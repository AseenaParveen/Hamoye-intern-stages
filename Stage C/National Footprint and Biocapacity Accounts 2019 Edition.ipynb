{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Blue> National Footprint and Biocapacity Accounts 2019 Edition\n",
    "    \n",
    "<font color = Blue>Dataset provides Ecological Footprint per capita data for years 1961-2016 in global hectares (gha).\n",
    "Ecological Footprint is a measure of how much area of biologically productive land and water an individual, population, or activity requires to produce all the resources it consumes and to absorb the waste it generates, using prevailing technology and resource management practices. The Ecological Footprint is measured in global hectares. Since trade is global, an individual or country's Footprint tracks area from all over the world. Without further specification, Ecological Footprint generally refers to the Ecological Footprint of consumption (rather than only production or export). Ecological Footprint is often referred to in short form as Footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color =Green> About this Dataset</b>\n",
    "<font color =Green>This data includes total and per capita national biocapacity, ecological footprint of consumption, ecological footprint of production and total area in hectares. This dataset, however, does not include any of our yield factors (national or world) nor any equivalence factors. To view these click here ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( r'D:\\Hamoye stage C\\footprint-nfa-2019-edition\\footprint-nfa-2019-edition\\NFA 2019 public_data.csv',low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72186 entries, 0 to 72185\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         72186 non-null  object \n",
      " 1   year            72186 non-null  int64  \n",
      " 2   country_code    72186 non-null  int64  \n",
      " 3   record          72186 non-null  object \n",
      " 4   crop_land       51714 non-null  float64\n",
      " 5   grazing_land    51714 non-null  float64\n",
      " 6   forest_land     51714 non-null  object \n",
      " 7   fishing_ground  51713 non-null  float64\n",
      " 8   built_up_land   51713 non-null  float64\n",
      " 9   carbon          51713 non-null  float64\n",
      " 10  total           72177 non-null  float64\n",
      " 11  QScore          72185 non-null  object \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72186, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country               0\n",
       "year                  0\n",
       "country_code          0\n",
       "record                0\n",
       "crop_land         20472\n",
       "grazing_land      20472\n",
       "forest_land       20472\n",
       "fishing_ground    20473\n",
       "built_up_land     20473\n",
       "carbon            20473\n",
       "total                 9\n",
       "QScore                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country            0.0\n",
       "year               0.0\n",
       "country_code       0.0\n",
       "record             0.0\n",
       "crop_land         28.4\n",
       "grazing_land      28.4\n",
       "forest_land       28.4\n",
       "fishing_ground    28.4\n",
       "built_up_land     28.4\n",
       "carbon            28.4\n",
       "total              0.0\n",
       "QScore             0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of missing values \n",
    "\n",
    "df.isna().mean().round(3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51481\n",
       "2A    10576\n",
       "2B    10096\n",
       "1A       16\n",
       "1B       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ 'QScore' ].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, we will drop the rows with missing values. \n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country           0\n",
       "year              0\n",
       "country_code      0\n",
       "record            0\n",
       "crop_land         0\n",
       "grazing_land      0\n",
       "forest_land       0\n",
       "fishing_ground    0\n",
       "built_up_land     0\n",
       "carbon            0\n",
       "total             0\n",
       "QScore            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      224\n",
       "1A       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ 'QScore' ].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =Red>An obvious change in our target variable after removing the missing values is that thereare only three classes left and from the distribution of the 3 classes, we can see that there is an obvious imbalance between the classes.<br></n>There are methods that can be applied to handle this imbalance such as oversampling and undersampling.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling involves increasing the number of instances in the class with fewer instances</n>while undersampling </n>involves reducing the data points in the class with more instances.</n>For now, we will convert this to a binary classification problem by combining class '2A'and '1A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ 'QScore' ] = df[ 'QScore' ].replace([ '1A' ], '2A' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ 'QScore' ].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating different dataframes\n",
    "df_2A = df[df.QScore== '2A' ]\n",
    "df_3A = df[df.QScore== '3A' ].sample( 350 )\n",
    "data_df = df_2A.append(df_3A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    350\n",
       "2A    240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = sklearn.utils.shuffle(data_df)\n",
    "data_df = data_df.reset_index(drop= True )\n",
    "data_df.shape\n",
    "data_df.QScore.value_counts() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['country_code' 'country' 'year'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-c467c07edfdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;34m'country_code'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'country'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'year'\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5282\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['country_code' 'country' 'year'] not found in axis\""
     ]
    }
   ],
   "source": [
    "data_df = data_df.drop(columns=[ 'country_code' , 'country' , 'year' ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop(columns= 'QScore' )\n",
    "y = data_df[ 'QScore' ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    236\n",
       "2A    177\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.3 , random_state= 0 )\n",
    "y_train.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is still an imbalance in the class distribution. For this, we use SMOTE only on thetraining data to handle this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "x_train.record = encoder.fit_transform(x_train.record)\n",
    "x_test.record = encoder.transform(x_test.record) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state= 1 )\n",
    "x_train_balanced, y_balanced = smote.fit_sample(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalised_train_df = scaler.fit_transform(x_train_balanced.drop(columns=[ 'record' ]))\n",
    "normalised_train_df = pd.DataFrame(normalised_train_df,\n",
    "columns=x_train_balanced.drop(columns=[ 'record' ]).columns)\n",
    "normalised_train_df[ 'record' ] = x_train_balanced[ 'record' ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reset_index(drop= True )\n",
    "normalised_test_df = scaler.transform(x_test.drop(columns=[ 'record' ]))\n",
    "normalised_test_df = pd.DataFrame(normalised_test_df,\n",
    "columns=x_test.drop(columns=[ 'record' ]).columns)\n",
    "normalised_test_df[ 'record' ] = x_test[ 'record' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalised_train_df, y_balanced)\n",
    "#returns\n",
    "LogisticRegression(C= 1.0 , class_weight= None , dual= False , fit_intercept= True ,\n",
    " intercept_scaling= 1 , l1_ratio= None , max_iter= 100 ,\n",
    " multi_class= 'auto' , n_jobs= None , penalty= 'l2' ,\n",
    " random_state= None , solver= 'lbfgs' , tol= 0.0001 , verbose= 0 ,\n",
    " warm_start= False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color= Blue> Measuring Classification Performance\n",
    "### *Cross-validation and accuracy*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.50437867, 50.32817889, 53.10657596, 59.50113379, 59.50113379])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(log_reg, normalised_train_df, y_balanced, cv= 5 , scoring= 'f1_macro' )\n",
    "scores* 100 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *K-Fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50.943396226415096, 54.0, 52.52525252525253, 55.00000000000001, 0.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "kf = KFold(n_splits= 5 )\n",
    "kf.split(normalised_train_df)\n",
    "f1_scores = []\n",
    "#run for every split \n",
    "\n",
    "for train_index, test_index in kf.split(normalised_train_df):\n",
    "    x_train, x_test = normalised_train_df.iloc[train_index],normalised_train_df.iloc[test_index]\n",
    "    y_train, y_test = y_balanced[train_index],y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(x_train, y_train)\n",
    "    f1_scores.append(f1_score(y_true=y_test, y_pred=model.predict(x_test),pos_label= '2A' )* 100 )\n",
    "f1_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Stratified K-Fold Cross Validation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55.769230769230774,\n",
       " 57.99999999999999,\n",
       " 60.60606060606061,\n",
       " 58.58585858585859,\n",
       " 48.484848484848484]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "skf = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1 )\n",
    "f1_scores = []\n",
    "#run for every split\n",
    "for train_index, test_index in skf.split(normalised_train_df, y_balanced):\n",
    "    x_train, x_test = np.array(normalised_train_df)[train_index],np.array(normalised_train_df)[test_index]\n",
    "    y_train, y_test = y_balanced[train_index], y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(x_train, y_train)\n",
    " #save result to list\n",
    "    f1_scores.append(f1_score(y_true=y_test, y_pred=model.predict(x_test), pos_label= '2A' )* 100) \n",
    "f1_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Leave One Out Cross Validation (LOOCV)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.96610169491526"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(LogisticRegression(), normalised_train_df, y_balanced, cv=loo,\n",
    " scoring= 'f1_macro' )\n",
    "average_score = scores.mean() * 100\n",
    "average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Confusion Matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2A', '2A', '2A', '3A', '2A', '2A', '2A', '2A', '3A', '2A', '3A',\n",
       "       '3A', '3A', '2A', '2A', '2A', '3A', '2A', '2A', '3A', '2A', '3A',\n",
       "       '2A', '2A', '3A', '2A', '2A', '2A', '2A', '3A', '2A', '2A', '2A',\n",
       "       '2A', '3A', '3A', '3A', '2A', '2A', '2A', '3A', '2A', '3A', '3A',\n",
       "       '2A', '2A', '2A', '3A', '2A', '2A', '3A', '2A', '3A', '2A', '2A',\n",
       "       '2A', '3A', '2A', '3A', '2A', '2A', '3A', '2A', '2A', '2A', '3A',\n",
       "       '2A', '3A', '3A', '3A', '2A', '2A', '2A', '3A', '2A', '2A', '3A',\n",
       "       '2A', '2A', '3A', '3A', '2A', '3A', '2A', '3A', '3A', '2A', '2A',\n",
       "       '3A', '2A', '2A', '2A', '2A', '3A', '2A', '2A', '3A', '2A', '2A',\n",
       "       '2A', '2A', '2A', '2A', '3A', '2A', '3A', '3A', '2A', '2A', '2A',\n",
       "       '3A', '3A', '3A', '2A', '2A', '2A', '2A', '3A', '2A', '2A', '3A',\n",
       "       '2A', '3A', '2A', '2A', '2A', '3A', '2A', '3A', '3A', '2A', '3A',\n",
       "       '2A', '2A', '3A', '3A', '3A', '3A', '2A', '3A', '2A', '3A', '2A',\n",
       "       '3A', '2A', '3A', '3A', '3A', '2A', '3A', '2A', '3A', '2A', '2A',\n",
       "       '3A', '2A', '2A', '2A', '3A', '2A', '2A', '2A', '2A', '2A', '3A',\n",
       "       '2A', '3A', '2A', '3A', '3A', '3A', '3A', '2A', '3A', '3A', '3A',\n",
       "       '3A'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score,confusion_matrix\n",
    "\n",
    "new_predictions = log_reg.predict(normalised_test_df)\n",
    "\n",
    "new_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 29],\n",
       "       [70, 44]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_predictions, labels=[ '2A' , '3A' ])\n",
    "\n",
    "cnf_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.33      0.54      0.41        63\n",
      "          3A       0.60      0.39      0.47       114\n",
      "\n",
      "    accuracy                           0.44       177\n",
      "   macro avg       0.46      0.46      0.44       177\n",
      "weighted avg       0.50      0.44      0.45       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=y_test, y_pred=new_predictions, labels=[ '2A' , '3A' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(normalised_test_df,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true=y_test, y_pred=new_predictions)\n",
    "print( 'Accuracy: {}' .format(round(accuracy* 100 ), 2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Precision* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 33\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_true=y_test, y_pred=new_predictions, pos_label= '2A' )\n",
    "print( 'Precision: {}' .format(round(precision* 100 ), 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *Recall* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 54\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true=y_test, y_pred=new_predictions, pos_label= '2A' )\n",
    "print( 'Recall: {}' .format(round(recall* 100 ), 2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *F1-Score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 41\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true=y_test, y_pred=new_predictions, pos_label= '2A' )\n",
    "print( 'F1: {}' .format(round(f1* 100 ), 2 )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tree-Based Methods and The Support Vector Machine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(normalised_train_df, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = dec_tree.predict(normalised_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6440677966101694"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tree.score(normalised_test_df,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          2A       0.50      0.54      0.52        63\n",
      "          3A       0.73      0.70      0.72       114\n",
      "\n",
      "    accuracy                           0.64       177\n",
      "   macro avg       0.62      0.62      0.62       177\n",
      "weighted avg       0.65      0.64      0.65       177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=new_predictions, labels=[ '2A' , '3A' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
