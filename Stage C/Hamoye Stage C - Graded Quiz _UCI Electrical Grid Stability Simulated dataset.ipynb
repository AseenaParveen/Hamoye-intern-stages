{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color =blue> UCI Electrical Grid Stability Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the Grid System\n",
    "Electrical grids require a balance between electricity supply and demand in order to be stable. Conventional systems achieve this balance through demand-driven electricity production. For future grids with a high share of inflexible (i.e., renewable) energy source, the concept of demand response is a promising solution. This implies changes in electricity consumption in relation to electricity price changes. In this work, we’ll build a binary classification model to predict if a grid is stable or unstable using the UCI Electrical Grid Stability Simulated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABOUT DATA\n",
    "\n",
    "It has 12 primary predictive features and two dependent variables.\n",
    "\n",
    "### Predictive features:\n",
    "\n",
    "'tau1' to 'tau4': the reaction time of each network participant, a real value within the range 0.5 to 10 ('tau1' corresponds to the supplier node, 'tau2' to 'tau4' to the consumer nodes);\n",
    "'p1' to 'p4': nominal power produced (positive) or consumed (negative) by each network participant, a real value within the range -2.0 to -0.5 for consumers ('p2' to 'p4'). As the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4);\n",
    "'g1' to 'g4': price elasticity coefficient for each network participant, a real value within the range 0.05 to 1.00 ('g1' corresponds to the supplier node, 'g2' to 'g4' to the consumer nodes; 'g' stands for 'gamma');\n",
    "\n",
    "### Dependent variables:\n",
    "\n",
    "'stab': the maximum real part of the characteristic differential equation root (if positive, the system is linearly unstable; if negative, linearly stable);\n",
    "'stabf': a categorical (binary) label ('stable' or 'unstable')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"D:\\Hamoye stage C\\UCI Electrical Grid Stability Simulated dataset\\Data_for_UCI_named.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4      stab     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   tau1    10000 non-null  float64\n",
      " 1   tau2    10000 non-null  float64\n",
      " 2   tau3    10000 non-null  float64\n",
      " 3   tau4    10000 non-null  float64\n",
      " 4   p1      10000 non-null  float64\n",
      " 5   p2      10000 non-null  float64\n",
      " 6   p3      10000 non-null  float64\n",
      " 7   p4      10000 non-null  float64\n",
      " 8   g1      10000 non-null  float64\n",
      " 9   g2      10000 non-null  float64\n",
      " 10  g3      10000 non-null  float64\n",
      " 11  g4      10000 non-null  float64\n",
      " 12  stab    10000 non-null  float64\n",
      " 13  stabf   10000 non-null  object \n",
      "dtypes: float64(13), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tau1     0\n",
       "tau2     0\n",
       "tau3     0\n",
       "tau4     0\n",
       "p1       0\n",
       "p2       0\n",
       "p3       0\n",
       "p4       0\n",
       "g1       0\n",
       "g2       0\n",
       "g3       0\n",
       "g4       0\n",
       "stab     0\n",
       "stabf    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color =blue>  No missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Stab column as it is direct correlated with 'stabf'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('stab',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  unstable  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unstable    6380\n",
       "stable      3620\n",
       "Name: stabf, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check distribution of target variable\n",
    "\n",
    "df['stabf'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #encode the categorical values\n",
    "# label = LabelEncoder()\n",
    "# df['stabf'] = label.fit(df['stabf']).transform(df['stabf'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop(columns=['stabf'])\n",
    "y= df['stabf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train sahpe :(8000, 12)\n",
      "y_train sahpe :(8000,)\n",
      "x_test sahpe :(2000, 12)\n",
      "y_test sahpe :(2000,)\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2 , random_state= 1 )\n",
    "print('x_train sahpe :{}'.format(x_train.shape))\n",
    "print('y_train sahpe :{}'.format(y_train.shape))\n",
    "print('x_test sahpe :{}'.format(x_test.shape))\n",
    "print('y_test sahpe :{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform train and test set using standard scaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train,y_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the scaled sets into a daataframe\n",
    "\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled, columns = x_train.columns)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, columns = x_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Red> *Train model using RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "# fit the train set \n",
    "rf.fit(x_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unstable', 'unstable', 'stable', ..., 'stable', 'stable',\n",
       "       'unstable'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict \n",
    "\n",
    "rf_pred = rf.predict(x_test_scaled)\n",
    "rf_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Measuring Model Performance for RandomForestClassifier***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *Accuracy of RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForestClassifier : 0.9290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,rf_pred)\n",
    "\n",
    "print( 'Accuracy of RandomForestClassifier : {:.4f}' .format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *Precision of RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of RandomForestClassifier: 91.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "#precision\n",
    "precision = precision_score(y_test, rf_pred, pos_label='stable')\n",
    "print('Precision of RandomForestClassifier: {:.2f}'.format((precision*100)))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *Recall for  RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for  RandomForestClassifier: 88\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, rf_pred, pos_label='stable')\n",
    "print('Recall for  RandomForestClassifier: {}'.format(round(recall*100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *F1 score of RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 90\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, rf_pred, pos_label='stable')\n",
    "print('F1: {}'.format(round(f1*100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *classification Report  for RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      stable     0.9191    0.8778    0.8980       712\n",
      "    unstable     0.9341    0.9573    0.9456      1288\n",
      "\n",
      "    accuracy                         0.9290      2000\n",
      "   macro avg     0.9266    0.9176    0.9218      2000\n",
      "weighted avg     0.9288    0.9290    0.9286      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:\\n', classification_report(y_test,rf_pred, digits =4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *confusion matrix for RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1233   55]\n",
      " [  87  625]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "rf_cnf_mat = confusion_matrix(y_test, rf_pred, labels=['unstable', 'stable'])\n",
    "print('Confusion Matrix:\\n', rf_cnf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *Training set score & Test set score RandomForestClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000\n",
      "Test set score: 0.929\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.3f}\".format(rf.score(x_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(rf.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Red> *Train model using XGBoost*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:48:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state = 1)\n",
    "\n",
    "# fit on train set\n",
    "xgb.fit(x_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "xgb_pred= xgb.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ***Measuring Model Performance for XGBoost***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost: 0.9455\n",
      "Precision of XGBoost: 94\n",
      "Recall for  XGBoost: 91\n",
      "F1: 92\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      stable     0.9351    0.9101    0.9224       712\n",
      "    unstable     0.9510    0.9651    0.9580      1288\n",
      "\n",
      "    accuracy                         0.9455      2000\n",
      "   macro avg     0.9430    0.9376    0.9402      2000\n",
      "weighted avg     0.9453    0.9455    0.9453      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1243   45]\n",
      " [  64  648]]\n"
     ]
    }
   ],
   "source": [
    "### Accuracy of XGBoost*\n",
    "\n",
    "accuracy = accuracy_score(y_test,xgb_pred)\n",
    "print( 'Accuracy of XGBoost: {:.4f}' .format(accuracy))\n",
    "\n",
    "## Precision of ExtraTreesClassifier*\n",
    "\n",
    "precision = precision_score(y_test, xgb_pred, pos_label='stable')\n",
    "print('Precision of XGBoost: {}'.format(round(precision*100), 2))  \n",
    "\n",
    "\n",
    "### <font color = blue> *Recall for ExtraTreesClassifier*\n",
    "\n",
    "recall = recall_score(y_test, xgb_pred, pos_label='stable')\n",
    "print('Recall for  XGBoost: {}'.format(round(recall*100), 2))\n",
    "\n",
    "### <font color = blue> *F1 score of ExtraTreesClassifier*\n",
    "\n",
    "f1 = f1_score(y_test, xgb_pred, pos_label='stable')\n",
    "print('F1: {}'.format(round(f1*100), 2))\n",
    "\n",
    "### <font color = blue> *classification Report  for ExtraTreesClassifier*\n",
    "\n",
    "print('Classification Report:\\n', classification_report(y_test,xgb_pred, digits =4))\n",
    "\n",
    "### <font color = blue> *confusion matrix for ExtraTreesClassifier*\n",
    "\n",
    "etc_cnf_mat = confusion_matrix(y_test, xgb_pred, labels=['unstable', 'stable'])\n",
    "print('Confusion Matrix:\\n', etc_cnf_mat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000\n",
      "Test set score: 0.946\n"
     ]
    }
   ],
   "source": [
    "### <font color = blue> *Training set score & Test set score ExtraTreesClassifier*\n",
    "print(\"Training set score: {:.3f}\".format(xgb.score(x_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(xgb.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Red> *Train model using LightGBM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm= LGBMClassifier(random_state = 1)\n",
    "\n",
    "#fit on train set\n",
    "lgbm.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "lgbm_pred= lgbm.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Measuring Model Performance for LightGBM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LightGBM: 0.9280\n",
      "Precision of LightGBM: 94\n",
      "Recall for  LightGBM: 85\n",
      "F1: 89\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      stable     0.9351    0.9101    0.9224       712\n",
      "    unstable     0.9510    0.9651    0.9580      1288\n",
      "\n",
      "    accuracy                         0.9455      2000\n",
      "   macro avg     0.9430    0.9376    0.9402      2000\n",
      "weighted avg     0.9453    0.9455    0.9453      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1250   38]\n",
      " [ 106  606]]\n",
      "Training set score: 0.998\n",
      "Test set score: 0.940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### <font color = blue> *Accuracy of LightGBM*\n",
    "\n",
    "accuracy = accuracy_score(y_test,lgbm_pred)\n",
    "print( 'Accuracy of LightGBM: {:.4f}' .format(accuracy))\n",
    "\n",
    "### <font color = blue> *Precision of ExtraTreesClassifier*\n",
    "\n",
    "precision = precision_score(y_test, lgbm_pred, pos_label='stable')\n",
    "print('Precision of LightGBM: {}'.format(round(precision*100), 2))  \n",
    "\n",
    "\n",
    "### <font color = blue> *Recall for ExtraTreesClassifier*\n",
    "\n",
    "recall = recall_score(y_test, lgbm_pred, pos_label='stable')\n",
    "print('Recall for  LightGBM: {}'.format(round(recall*100), 2))\n",
    "\n",
    "### <font color = blue> *F1 score of ExtraTreesClassifier*\n",
    "\n",
    "f1 = f1_score(y_test, lgbm_pred, pos_label='stable')\n",
    "print('F1: {}'.format(round(f1*100), 2))\n",
    "\n",
    "### <font color = blue> *classification Report  for ExtraTreesClassifier*\n",
    "\n",
    "print('Classification Report:\\n', classification_report(y_test,xgb_pred, digits =4))\n",
    "\n",
    "### <font color = blue> *confusion matrix for ExtraTreesClassifier*\n",
    "\n",
    "etc_cnf_mat = confusion_matrix(y_test, lgbm_pred, labels=['unstable', 'stable'])\n",
    "print('Confusion Matrix:\\n', etc_cnf_mat)\n",
    "\n",
    "### <font color = blue> *Training set score & Test set score ExtraTreesClassifier*\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(lgbm.score(x_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lgbm.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Red> *Train model using extra trees classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(random_state = 1)\n",
    "\n",
    "#fit on the trining set\n",
    "etc.fit(x_train_scaled,y_train)\n",
    "\n",
    "etc_pred = etc.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Measuring Model Performance for ExtraTreesClassifier***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> *Accuracy of ExtraTreesClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ExtraTreesClassifier: 0.9280\n",
      "Precision of ExtraTreesClassifierM: 94\n",
      "Recall for  ExtraTreesClassifier: 85\n",
      "F1: 89\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      stable     0.9351    0.9101    0.9224       712\n",
      "    unstable     0.9510    0.9651    0.9580      1288\n",
      "\n",
      "    accuracy                         0.9455      2000\n",
      "   macro avg     0.9430    0.9376    0.9402      2000\n",
      "weighted avg     0.9453    0.9455    0.9453      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1250   38]\n",
      " [ 106  606]]\n",
      "Training set score: 1.000\n",
      "Test set score: 0.928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### <font color = blue> *Accuracy of ExtraTreesClassifier*\n",
    "\n",
    "accuracy = accuracy_score(y_test,etc_pred)\n",
    "print( 'Accuracy of ExtraTreesClassifier: {:.4f}' .format(accuracy))\n",
    "\n",
    "### <font color = blue> *Precision of ExtraTreesClassifier*\n",
    "\n",
    "precision = precision_score(y_test, etc_pred, pos_label='stable')\n",
    "print('Precision of ExtraTreesClassifierM: {}'.format(round(precision*100), 2))  \n",
    "\n",
    "\n",
    "### <font color = blue> *Recall for ExtraTreesClassifier*\n",
    "\n",
    "recall = recall_score(y_test, etc_pred, pos_label='stable')\n",
    "print('Recall for  ExtraTreesClassifier: {}'.format(round(recall*100), 2))\n",
    "\n",
    "### <font color = blue> *F1 score of ExtraTreesClassifier*\n",
    "\n",
    "f1 = f1_score(y_test, etc_pred, pos_label='stable')\n",
    "print('F1: {}'.format(round(f1*100), 2))\n",
    "\n",
    "### <font color = blue> *classification Report  for ExtraTreesClassifier*\n",
    "\n",
    "print('Classification Report:\\n', classification_report(y_test,xgb_pred, digits =4))\n",
    "\n",
    "### <font color = blue> *confusion matrix for ExtraTreesClassifier*\n",
    "\n",
    "etc_cnf_mat = confusion_matrix(y_test, etc_pred, labels=['unstable', 'stable'])\n",
    "print('Confusion Matrix:\\n', etc_cnf_mat)\n",
    "\n",
    "### <font color = blue> *Training set score & Test set score ExtraTreesClassifier*\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(etc.score(x_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(etc.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = Red> *Improving ExtraTreesClassifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 17\n",
    "\n",
    "### <font color = Red>To improve the Extra Trees Classifier, you will use the following parameters (number of estimators, minimum number of samples, minimum number of samples for leaf node and the number of features to consider when looking for the best split) for the hyperparameter grid needed to run a Randomized Cross Validation Search (RandomizedSearchCV).\n",
    "\n",
    "### <font color = Red> n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "### <font color = Red>min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "### <font color = Red>min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "### <font color = Red>max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "### <font color = Red>hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "### <font color = Red>'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "### <font color = Red>'min_samples_split': min_samples_split,\n",
    "\n",
    "### <font color = Red>'max_features': max_features}\n",
    "\n",
    "### <font color = Red>Using the ExtraTreesClassifier as your estimator with cv=5, n_iter=10, scoring = 'accuracy', n_jobs = -1, verbose = 1 and random_state = 1. What are the best hyperparameters from the randomized search CV?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of hyperparameters\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None] \n",
    "\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "                       'min_samples_split': min_samples_split,\n",
    "\n",
    "                       'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Random Search for hyper-parameter optimization\n",
    "randomscv = RandomizedSearchCV(estimator = etc, param_distributions = hyperparameter_grid, \n",
    "                          cv = 5, n_iter = 10, scoring = 'accuracy', \n",
    "                          n_jobs = -1, verbose = 1, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#fit on the training data\n",
    "search = randomscv.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters from the randomized search CV: {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}\n"
     ]
    }
   ],
   "source": [
    "# best hyperparameters from the randomized search CV\n",
    "\n",
    "print('best hyperparameters from the randomized search CV: {}'.format(randomscv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = Red > Answer for Question 17 : best hyperparameters from the randomized search CV: \n",
    "### <font color = Blue>{'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9241249999999999"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get best score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(max_features=None, min_samples_leaf=8, n_estimators=1000,\n",
       "                     random_state=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate ExtraTreesClassifier on test set using  best params\n",
    "extra_etc = ExtraTreesClassifier(max_features = None, \n",
    "                            min_samples_leaf= 8,\n",
    "                            min_samples_split= 2,\n",
    "                            n_estimators= 1000, \n",
    "                            random_state = 1)\n",
    "\n",
    "#fit on train set\n",
    "extra_etc.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test set\n",
    "extra_etc_pred = extra_etc.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Measuring Model Performance for ExtraTreesClassifier***\n",
    "\n",
    "### <font color = blue> *Accuracy of ExtraTreesClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of extra_etc: 0.9270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test,extra_etc_pred)\n",
    "print( 'Accuracy of extra_etc: {:.4f}' .format((accuracy)))\n",
    "\n",
    "# ### <font color = blue> *Precision of ExtraTreesClassifier*\n",
    "\n",
    "# # precision = precision_score(y_test, extra_etc_pred, pos_label='stable')\n",
    "# # print('Precision of ExtraTreesClassifier: {}'.format(round(precision*100), 2))  \n",
    " \n",
    "\n",
    "\n",
    "# ### <font color = blue> *Recall for ExtraTreesClassifier*\n",
    "\n",
    "# recall = recall_score(y_test, extra_etc_pred, pos_label='stable')\n",
    "# print('Recall for  ExtraTreesClassifier: {}'.format(round(recall*100), 2))\n",
    "\n",
    "# ### <font color = blue> *F1 score of ExtraTreesClassifier*\n",
    "\n",
    "# f1 = f1_score(y_test, extra_etc_pred, pos_label='stable')\n",
    "# print('F1: {}'.format(round(f1*100), 2))\n",
    "\n",
    "# ### <font color = blue> *classification Report  for ExtraTreesClassifier*\n",
    "\n",
    "# print('Classification Report:\\n', classification_report(y_test,extra_etc_pred, digits =4))\n",
    "\n",
    "# ### <font color = blue> *confusion matrix for ExtraTreesClassifier*\n",
    "\n",
    "# etc_cnf_mat = confusion_matrix(y_test, extra_etc_pred, labels=['unstable', 'stable'])\n",
    "# print('Confusion Matrix:\\n', etc_cnf_mat)\n",
    "\n",
    "# ### <font color = blue> *Training set score & Test set score ExtraTreesClassifier*\n",
    "\n",
    "# print(\"Training set score: {:.3f}\".format(extra_etc.score(x_train_scaled, y_train)))\n",
    "# print(\"Test set score: {:.3f}\".format(extra_etc.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 18\n",
    "## <font color = Red> Train a new ExtraTreesClassifier Model with the new Hyperparameters from the RandomizedSearchCV (with random_state = 1). Is the accuracy of the new optimal model higher or lower than the initial ExtraTreesClassifier model with no hyperparameter tuning?\n",
    "    \n",
    "### <font color = Blue> Answer : Accuracy of extra_etc with hyper tuning : 0.9270 < Accuracy of ExtraTreesClassifier: 0.9280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 20\n",
    "\n",
    "### <font color = Red> Find the feature importance using the optimal ExtraTreesClassifier model. Which features are the most and least important respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p1</th>\n",
       "      <td>0.039507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p2</th>\n",
       "      <td>0.040371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p4</th>\n",
       "      <td>0.040579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p3</th>\n",
       "      <td>0.040706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g1</th>\n",
       "      <td>0.089783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g2</th>\n",
       "      <td>0.093676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g4</th>\n",
       "      <td>0.094019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g3</th>\n",
       "      <td>0.096883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau3</th>\n",
       "      <td>0.113169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau4</th>\n",
       "      <td>0.115466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau1</th>\n",
       "      <td>0.117397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau2</th>\n",
       "      <td>0.118445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "p1    0.039507\n",
       "p2    0.040371\n",
       "p4    0.040579\n",
       "p3    0.040706\n",
       "g1    0.089783\n",
       "g2    0.093676\n",
       "g4    0.094019\n",
       "g3    0.096883\n",
       "tau3  0.113169\n",
       "tau4  0.115466\n",
       "tau1  0.117397\n",
       "tau2  0.118445"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = X.columns\n",
    "\n",
    "# features importance\n",
    "feat_importance = pd.DataFrame(etc.feature_importances_,index=feature)\n",
    "feat = feat_importance.sort_values(0)\n",
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most important feature: 0    tau2\n",
      "dtype: object\n",
      "least important feature: 0    p1\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# most important feature\n",
    "print('most important feature: {}'.format(feat.idxmax()))\n",
    "\n",
    "# least important feature\n",
    "print('least important feature: {}'.format(feat.idxmin()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = Red> Answer for Question20  : most important feature:    tau2\n",
    " \n",
    "### <font color = Red> least important feature:    p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 1\n",
    "## <font color = Red>You are working on a spam classification system using regularized logistic regression. “Spam” is a positive class (y = 1) and “not spam” is the negative class (y = 0). You have trained your classifier and there are n = 2000 examples in the test set. The confusion matrix of predicted class vs. actual class is:</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Spam</th>\n",
       "      <th>Actual Not Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Spam</th>\n",
       "      <td>355</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Not Spam</th>\n",
       "      <td>45</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actual Spam  Actual Not Spam\n",
       "Predicted Spam              355             1480\n",
       "Predicted Not Spam           45              120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the correlation matrix\n",
    "corr_data = [[355, 1480], [45, 120]]\n",
    "corr_matrix = pd.DataFrame(corr_data, index=['Predicted Spam', 'Predicted Not Spam'],columns=['Actual Spam', 'Actual Not Spam'])\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the F1 score of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3177"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#calculate f1_score\n",
    "tp = 355\n",
    "tn = 120\n",
    "fp = 1480\n",
    "fn = 45\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall= tp / (tp + fn)\n",
    "f1_score=  2*((precision* recall)/(precision+recall))\n",
    "round(f1_score,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 14\n",
    "\n",
    "## <font color = Red> What is the accuracy on the test set using the random forest classifier? In 4 decimal places.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ExtraTreesClassifier: 0.9290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "# fit the train set \n",
    "rf.fit(x_train_scaled,y_train)\n",
    "\n",
    "rf_pred = rf.predict(x_test_scaled)\n",
    "accuracy = accuracy_score(y_test,rf_pred)\n",
    "\n",
    "print( 'Accuracy of ExtraTreesClassifier: {:.4f}' .format((accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 15\n",
    "\n",
    "## <font color = Red> What is the accuracy on the test set using the xgboost classifier? In 4 decimal places..</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ExtraTreesClassifier: 0.9455\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state = 1)\n",
    "\n",
    "# fit on train set\n",
    "xgb.fit(x_train_scaled,y_train)\n",
    "xgb_pred = xgb.predict(x_test_scaled)\n",
    "accuracy = accuracy_score(y_test,xgb_pred)\n",
    "print( 'Accuracy of ExtraTreesClassifier: {:.4f}' .format((accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = Red> Question 16\n",
    "\n",
    "## <font color = Red> What is the accuracy on the test set using the LGBM classifier? In 4 decimal places.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ExtraTreesClassifier: 0.9280\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm= LGBMClassifier(random_state = 1)\n",
    "\n",
    "#fit on train set\n",
    "lgbm.fit(x_train_scaled, y_train)\n",
    "lgbm_pred = etc.predict(x_test_scaled)\n",
    "accuracy = accuracy_score(y_test,lgbm_pred)\n",
    "\n",
    "print( 'Accuracy of ExtraTreesClassifier: {:.4f}' .format((accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
